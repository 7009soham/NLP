# 03. Traditional NLP Methods

## Overview
Classical approaches to NLP that rely on linguistic rules and statistical methods.

## Topics

### 3.1 Text Representation
- Bag of Words (BoW)
- TF-IDF (Term Frequency-Inverse Document Frequency)
- N-gram models
- One-hot encoding

### 3.2 Language Models
- Statistical Language Models
- N-gram Language Models
- Smoothing techniques (Laplace, Good-Turing)
- Perplexity and evaluation

### 3.3 Text Classification
- Naive Bayes Classifier
- Logistic Regression
- Support Vector Machines (SVM)
- Decision Trees and Random Forests

### 3.4 Information Extraction
- Named Entity Recognition (Rule-based)
- Part-of-Speech Tagging
- Chunking and Parsing
- Dependency Parsing

### 3.5 Topic Modeling
- Latent Semantic Analysis (LSA)
- Latent Dirichlet Allocation (LDA)
- Non-negative Matrix Factorization (NMF)

### 3.6 Similarity and Distance Metrics
- Cosine similarity
- Euclidean distance
- Jaccard similarity
- Edit distance (Levenshtein)

## Learning Objectives
- Understand classical NLP approaches
- Implement statistical language models
- Apply traditional ML methods to text
- Perform topic modeling and information extraction

## Resources
- Implementation examples
- Comparison with modern methods
- Datasets for practice

# 04. Word Embeddings

## Overview
Learn about dense vector representations of words that capture semantic relationships.

## Topics

### 4.1 Introduction to Word Embeddings
- Limitations of traditional representations
- Distributional semantics
- Word vector properties
- Embedding space visualization

### 4.2 Word2Vec
- CBOW (Continuous Bag of Words)
- Skip-gram model
- Negative sampling
- Hierarchical softmax
- Training and using Word2Vec

### 4.3 GloVe
- Global Vectors for Word Representation
- Co-occurrence matrix
- GloVe vs Word2Vec
- Pre-trained GloVe models

### 4.4 FastText
- Subword information
- Character n-grams
- Handling out-of-vocabulary words
- Multilingual embeddings

### 4.5 Evaluation and Analysis
- Word similarity tasks
- Word analogies
- Intrinsic vs extrinsic evaluation
- Bias in word embeddings

### 4.6 Contextualized Embeddings (Introduction)
- Context-independent vs context-dependent
- ELMo (Embeddings from Language Models)
- Bridge to modern approaches

## Learning Objectives
- Understand word embedding concepts
- Train and use Word2Vec, GloVe, and FastText
- Evaluate embedding quality
- Apply embeddings to downstream tasks

## Resources
- Pre-trained embedding models
- Visualization tools
- Training scripts and examples
